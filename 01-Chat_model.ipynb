{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4773c-7113-43ee-b85e-2b331d5ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NVIDIA_API_KEY=nvapi-1234567890abcdef   # replace with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca158aa-1ac5-4278-9ef4-fe611f25aaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -q python-dotenv langchain_nvidia_ai_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e237e25-de00-490d-bad2-fb3a4a352a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Get NVIDIA's API Key: the API key should start with \"nvapi-\"\n",
    "if not os.getenv(\"NVIDIA_API_KEY\"):\n",
    "    load_dotenv(find_dotenv()) # read .env file\n",
    "    apikey = os.getenv('NVIDIA_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9980d6-808c-4103-b706-f2a6303774be",
   "metadata": {},
   "source": [
    "## Working with NVIDIA NIM\n",
    "คุณสามารถทดสอบ LLM model ต่างๆ ที่มีใน NVIDIA NIM ได้อย่างง่ายดาย โดยทำได้ 2 รูปแบบคือ\n",
    "1. ติดตั้ง NVIDIA NIM ของตนเอง ซึ่งคุณสามารถติดตั้ง NIM ได้ทั้งบน on-premise หรือบน Cloud instance ของคุณเอง เช่น AWS EC2 ก็ได้\n",
    "2. ทำการเรียกใช้ NIM โดยตรงจาก NVIDIA Playground (https://build.nvidia.com) ซึ่งเปิดให้ใช้งานจำนวนหนึ่ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af77a2f-3715-4ede-87d8-11e3ea322bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Thailand is Bangkok.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    # base_url=\"http://192.134.1.100:30644/v1\", # ระบุ NIM endpoint ของคุณ, ถ้าไม่ระบุจะวิ่งไปที่ NVIDIA Playground\n",
    "    model=\"meta/llama-3.1-8b-instruct\",        # ระบุ LLM model ที่ต้องการใช้\n",
    "    temperature=0.1, \n",
    "    max_tokens=1000, \n",
    "    top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of Thailand?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22120fb0-9ec7-4307-9e7a-48923ba62939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ความรู้เท่าหางอึ่ง หมายถึง ความรู้ที่มีความลึกและกว้างขวางมากจนไม่สามารถจดจำหรือจำได้\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"จงอธิบายความหมายของคำว่า ความรู้เท่าหางอึ่ง\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f57fe1-c9a1-4033-8b03-114af3fbb2af",
   "metadata": {},
   "source": [
    "## Stream, Batch, and Async\n",
    "การ call model สามารถเรียกได้ทั้งแบบ Stream, Batch และ Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a0062-de86-4a35-a49e-b733e8a65cc9",
   "metadata": {},
   "source": [
    "#### การประมวลผลแบบ Batch \n",
    "โดยการจัดเตรียม prompt จำนวนหลายๆ prompt ให้อยู่ในรูปแบบ List แล้วส่งเข้ามาประมวลผลในครั้งเดียว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6736c3f6-cdbc-4d06-8165-e8c058277792",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='2 × 3 = 6', response_metadata={'role': 'assistant', 'content': '2 × 3 = 6', 'token_usage': {'prompt_tokens': 18, 'total_tokens': 25, 'completion_tokens': 7}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-771db744-ec33-41fe-b0ef-e414b78f49c1-0', role='assistant'),\n",
       " AIMessage(content='2 * 6 = 12', response_metadata={'role': 'assistant', 'content': '2 * 6 = 12', 'token_usage': {'prompt_tokens': 18, 'total_tokens': 25, 'completion_tokens': 7}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-995d4280-9d55-4e61-9ba5-3d0e12a9795b-0', role='assistant')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(llm.batch([\"What's 2*3?\", \"What's 2*6?\"]))\n",
    "# Or via the async API\n",
    "# await llm.abatch([\"What's 2*3?\", \"What's 2*6?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2789cf3-325d-430e-942a-aab866668f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### การประมวลผลแบบ Stream จะส่ง output ออกมาทีละ Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adb4191-eb44-45b9-866a-1526f9497816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Se|ag|ulls| are| incredibly| skilled| and| long|-distance| f|liers|.| Their| flying| abilities| are| adapted| to| their| marine| environment|,| and| they| can| cover| impressive| distances| in| search| of| food|,| shelter|,| and| breeding| grounds|.\n",
      "\n",
      "|The| distance| a| se|ag|ull| can| fly| in| one| day| depends| on| various| factors|,| such| as|:\n",
      "\n",
      "|1|.| **|Species|**:| Different| se|ag|ull| species| have| varying| flying| abilities|.| For| example|,| the| H|erring| G|ull| (|L|arus| argent|atus|)| is| known| for| its| impressive| long|-distance| flights|,| while| the| Laugh|ing| G|ull| (|Le|uc|op|ha|eus| atr|ic|illa|)| is| more| adapted| to| coastal| areas| and| may| not| fly| as| far|.\n",
      "|2|.| **|Weather| conditions|**:| Wind|,| temperature|,| and| humidity| can| significantly| impact| a| se|ag|ull|'s| flying| ability|.| Strong| head|w|inds|,| for| instance|,| can| reduce| their| flying| range|.\n",
      "|3|.| **|Food| availability|**:| Se|ag|ulls| may| fly| longer| distances| to| find| food|,| especially| if| their| usual| sources| are| scarce|.\n",
      "|4|.| **|Migration| patterns|**:| Some| se|ag|ull| species| are| migr|atory|,| and| their| daily| flying| distances| can| be| influenced| by| their| migration| routes| and| schedules|.\n",
      "\n",
      "|That| being| said|,| here| are| some| estimates| of| the| maximum| daily| flying| distances| for| different| se|ag|ull| species|:\n",
      "\n",
      "|*| **|H|erring| G|ull|**:| Up| to| |500|-|600| km| (|310|-|373| miles|)| in| a| single| day|,| with| some| individuals| reportedly| flying| over| |1|,|000| km| (|621| miles|)| in| a| |24|-hour| period|.\n",
      "|*| **|Less|er| Black|-backed| G|ull|**:| Up| to| |400|-|500| km| (|250|-|310| miles|)| per| day|.\n",
      "|*| **|Laugh|ing| G|ull|**:| Typically| flies| shorter| distances|,| around| |100|-|200| km| (|62|-|124| miles|)| per| day|.\n",
      "\n",
      "|Keep| in| mind| that| these| estimates| are| based| on| observations| and| may| not| reflect| the| actual| flying| distances| of| individual| se|ag|ulls|.| Se|ag|ulls| are| highly| adaptable| and| can| adjust| their| flying| patterns| to| suit| their| specific| needs| and| environmental| conditions|.\n",
      "\n",
      "|Now|,| imagine| a| se|ag|ull| soaring| overhead|,| effortlessly| gl|iding| across| the| sky|,| covering| hundreds| of| kilometers| in| a| single| day|.| It|'s| a| remarkable| sight|,| isn|'t| it|?||"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"How far can a seagull fly in one day?\"):\n",
    "    # Show the token separations\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69580eba-7957-4060-8977-cc317985e652",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### การประมวลผลแบบ Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec3272cd-6beb-4300-9b15-63a1ca95fc04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| incredible| monarch| butterfly| migration|!\n",
      "\n",
      "|The| monarch| butterfly| migration| is| a| remarkable| phenomenon| that| takes| place| every| year|,| with| millions| of| monarch|s| traveling| from| Canada| and| the| United| States| to| Mexico| and| California|.| The| duration| of| the| migration| varies| depending| on| the| location| and| the| specific| monarch| population|,| but| here| are| some| general| guidelines|:\n",
      "\n",
      "|**|From| Canada| and| the| United| States| to| Mexico|:|**\n",
      "\n",
      "|*| The| monarch|s| that| migrate| from| Canada| and| the| United| States| to| Mexico| typically| begin| their| journey| in| late| summer| or| early| fall|,| around| August| or| September|.\n",
      "|*| The| journey| can| take| anywhere| from| |2| to| |4| weeks|,| covering| a| distance| of| around| |3|,|000| to| |4|,|000| miles| (|4|,|800| to| |6|,|400| kilometers|).\n",
      "|*| The| monarch|s| fly| at| an| average| speed| of| about| |10|-|15| miles| (|16|-|24| kilometers|)| per| hour|,| with| some| individuals| reaching| speeds| of| up| to| |30| miles| (|48| kilometers|)| per| hour|.\n",
      "\n",
      "|**|From| California| to| Mexico|:|**\n",
      "\n",
      "|*| The| monarch|s| that| migrate| from| California| to| Mexico| typically| begin| their| journey| in| late| October| or| early| November|.\n",
      "|*| The| journey| can| take| around| |2|-|3| weeks|,| covering| a| distance| of| around| |1|,|000| to| |1|,|500| miles| (|1|,|600| to| |2|,|400| kilometers|).\n",
      "\n",
      "|**|From| Mexico| to| California|:|**\n",
      "\n",
      "|*| The| monarch|s| that| migrate| from| Mexico| to| California| typically| begin| their| journey| in| late| February| or| early| March|.\n",
      "|*| The| journey| can| take| around| |2|-|4| weeks|,| covering| a| distance| of| around| |1|,|000| to| |1|,|500| miles| (|1|,|600| to| |2|,|400| kilometers|).\n",
      "\n",
      "|It|'s| worth| noting| that| the| monarch|s| don|'t| migrate| alone|;| they| travel| in| large| groups|,| often| with| multiple| generations| of| monarch|s| making| the| journey|.| The| monarch|s| that| migrate| from| Canada| and| the| United| States| to| Mexico| are| typically| the| fourth| or| fifth| generation| of| the| year|,| while| the| monarch|s| that| migrate| from| California| to| Mexico| are| typically| the| third| or| fourth| generation|.\n",
      "\n",
      "|The| monarch|s|'| incredible| migration| is| made| possible| by| their| remarkable| ability| to| navigate| using| the| position| of| the| sun|,| the| Earth|'s| magnetic| field|,| and| even| the| scent| of| certain| plants|.||"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\n",
    "    \"How long does it take for monarch butterflies to migrate?\"\n",
    "):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c230f-1e56-49f3-a10b-188bfb9ac8da",
   "metadata": {},
   "source": [
    "**เรียกดูว่าในระบบมี model อะไรให้ใช้งานบ้าง**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef9bbd-38a6-461c-99bd-5fa9322aa731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1188a-afe9-4a7f-8c85-8a62e13e14ae",
   "metadata": {},
   "source": [
    "### General Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280ff812-310c-42ef-93b0-58f13cee9dae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Fred, and I'm a helpful AI assistant. I'm here to assist you with any questions or tasks you may have. What can I help you with today?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful AI assistant named Fred.\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for txt in chain.stream({\"input\": \"What's your name?\"}):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ee0a8-e8dc-4684-bce9-9683d7e24e57",
   "metadata": {},
   "source": [
    "คุณอาจจะทดสองเปลี่ยนชื่อดู จะเห็นว่าตัว Chat สามารถจดจำชื่อคุณได้\n",
    "อย่างไรก็ตาม การเรียก Chat แบบนี้เป็นการทำงานแบบ Single turn ซึ่งจะจดจำข้อมูลได้เฉพาะภายใน prompt เดียวกันเท่านั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20112e8-ff34-41c8-9948-99271167d94c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Generation\n",
    "ถัดไปเราจะลองเปลี่ยน model จาก Llama-3.1 มาเป็นโมเดลอีกตัว คือ codellama-70b ซึ่งใช้สำหรับ generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81b8bfdf-b028-4214-85df-643b0bfc4307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def fact(n):\n",
      "    if n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * fact(n-1)\n",
      "\n",
      "def mysquare(x):\n",
      "    return x * x\n",
      "\n",
      "# Test the functions\n",
      "print(fact(5))  # Output: 120\n",
      "print(mysquare(5))  # Output: 25"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert coding AI. Respond only in valid python; no narration whatsoever.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | ChatNVIDIA(model=\"meta/codellama-70b\") | StrOutputParser()\n",
    "\n",
    "query = \"\"\"give me python code to define 2 function - \\\n",
    "        fact(n) will compute the value of n factorial, \\\n",
    "        mysquare(x) will compute square of x, \\\n",
    "        then show show sample to use those functions.\n",
    "        \"\"\"\n",
    "\n",
    "for txt in chain.stream({\"input\": query}):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ede2cb-6d43-46b5-9a7b-21f3df341d25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# test code generated by model\n",
    "\n",
    "def fact(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * fact(n-1)\n",
    "\n",
    "def mysquare(x):\n",
    "    return x * x\n",
    "\n",
    "# Test the functions\n",
    "print(fact(5))  # Output: 120\n",
    "print(mysquare(5))  # Output: 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a884b7e-97b9-4ed5-a938-7e1edd54832e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch+NGC on Kubernetes Operator 240702161026",
   "language": "python",
   "name": "jupyter-eg-kernel-k8s-cmjkop-ngc-py-1i26n1a82"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
